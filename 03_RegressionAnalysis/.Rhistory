CPS<- CPS[complete.cases(CPS$PRTAGE), ]
CPS <- cps_2212
CPS<- CPS[complete.cases(CPS$PRTAGE), ]
CPS<- subset(CPS,CPS$PRTAGE>=16)
CPS$PUWK<-ifelse(CPS$PUWK=="1", "TRUE","FALSE")
names(CPS)[names(CPS) == "PUWK"] <- "employed"
CPS$PUWK<-ifelse(CPS$PUWK=="1", "TRUE","FALSE")
CPS$Unemployed<- ifelse(CPS$employed=="FALSE"&CPS$PULK=="-1", "TRUE", "FALSE")
CPS$LaborForce<-ifelse(CPS$employed=="TRUE"|CPS$Unemployed=="TRUE", "IN", "NOT_IN")
sum(CPS$Unemployed=="TRUE")/sum(CPS$LaborForce=="IN")
#[1] 0.3554322
sum(CPS$LaborForce=="IN")/80587
#[1] 0.8585628
## PLACE CODE HERE
unemployed <- unemployed[, c("PEEDUCA", "PRUNEDUR")]
## PLACE CODE HERE
unemployed <- CPS[, c("PEEDUCA", "PRUNEDUR")]
grouped_data <- aggregate(PRUNEDUR ~ PEEDUCA, data = unemployed, mean)
barplot(height = grouped_data$PRUNEDUR, names.arg = grouped_data$PEEDUCA, xlab = "Education Level", ylab = "Average Duration")
load("~/Downloads/cps_2212.rda")
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
cps_omit <- subset(cps_2212,!is.na(PRTAGE) )
df <- (cps_omit$PRTAGE[cps_omit$PRTAGE > 14 & cps_omit$PRTAGE < 65])
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
cps_omit <- subset(cps_2212,!is.na(PRTAGE) )
df <- (cps_omit$PRTAGE[cps_omit$PRTAGE > 14 & cps_omit$PRTAGE < 65])
## PLACE CODE HERE
library(dplyr)
recode(cps_omit$PUWK, '1'=TRUE, .default=FALSE)
cps_omit <- rename(cps_omit, employed = PUWK)
View(cps_omit)
## PLACE CODE HERE
dur <- function(d, em){
mean <- vector(mode="numeric", length=0)
j=1
for (i in 1:length(d)){
if(!em[i]){
if(d[i] != -1){
mean[j]<-d[i]
j=j+1
}}
}
return (mean)
}
durations <- dur(cps_omit$PRUNEDUR, cps_omit$employed)
print(mean(durations))
print(median(durations))
#print(hist(durations, breaks=6))
#31-38 no HS diploma
#39 high school
#40-43 college
#graduate degree
get_edu_group <- function(d, em, edu, min, max){
mean <- vector(mode="numeric", length=0)
j=1
for (i in 1:length(d)){
if(!em[i]){
if(d[i] != -1){
if(edu[i]>=min){
if(edu[i]<=max){
mean[j]<-d[i]
j=j+1
}}}}
}
return (mean)
}
no_hs <- get_edu_group(cps_omit$PRUNEDUR, cps_omit$employed, cps_omit$PEEDUCA, 31, 38)
hs <- get_edu_group(cps_omit$PRUNEDUR, cps_omit$employed, cps_omit$PEEDUCA, 39, 39)
under <- get_edu_group(cps_omit$PRUNEDUR, cps_omit$employed, cps_omit$PEEDUCA, 40, 43)
grad <- get_edu_group(cps_omit$PRUNEDUR, cps_omit$employed, cps_omit$PEEDUCA, 44, 46)
print(mean(no_hs))
print(mean(hs))
print(mean(under))
print(mean(grad))
load("~/Downloads/cps_2212.rda")
knitr::opts_chunk$set(echo = TRUE)
load("~/Downloads/cps_2212.rda")
CPS2 <- na.omit (cps_2212)
Oldest <- CPS2 [CPS2$AGE >= 16 ]
"Using the first set of code I was able to create a new data set from the original that excluded the NA values. I then created one more that took the newly created dataset and made the minimum age 16. This eliminated 44902 rows."
load("~/Downloads/cps_2212.rda")
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
Old <- na.omit(cps_2212) #takes out all NA in the whole data set
New <- Old[(Old$PRTAGE > 16),] #filters the data by age
## PLACE CODE HERE
New$EmployedLastWeek <- ifelse(New$PUWK == 1, TRUE, FALSE) #employed people answered 1, they are not unemployed
New$LaidOffLastWeek <- ifelse(New$PULAY == 1, TRUE, FALSE) #People laid off last week answered 1
New$AttemptingToFindWork <- ifelse(New$PULK == 1, TRUE, FALSE) #Those taking actions answered 1
New$ActionsToFindWork <- ifelse(New$PELKM1 >11, FALSE, TRUE) # answers 12 and 13 are not looking for work
New$AbleToWorkLastWeek <- ifelse(New$PELKAVL == 1, TRUE, FALSE) #If you were able to work last week, you answered 1
## PLACE CODE HERE
New$Unemployed <- ifelse(New$EmployedLastWeek == FALSE & New$LaidOffLastWeek == TRUE & New$AttemptingToFindWork == TRUE & New$ActionsToFindWork == TRUE & New$AbleToWorkLastWeek == TRUE,TRUE, FALSE)
#people must be not working last week or laid off last week
U <- sum(New$Unemployed)
P <- nrow(New)
uRate <- U/P
#unemployed people/count of everyone
## PLACE CODE HERE
UNew <- New[(New$Unemployed == TRUE),]
M <- mean(UNew$PRUNEDUR)
ME <- median(UNew$PRUNEDUR)
summary(UNew$PRUNEDUR)
hist(UNew$PRUNEDUR)
UHSD <- UNew[(UNew$PRUNEDUR == 39),]
UBD <- UNew[(UNew$PRUNEDUR == 43),]
UPD <- UNew[(UNew$PRUNEDUR == 45),]
MHSD <- mean(UHSD$PRUNEDUR)
MUBD <- mean(UBD$PRUNEDUR)
MUPD <- mean(UPD$PRUNEDUR)
load("~/Downloads/cps_2212.rda")
knitr::opts_chunk$set(echo = TRUE)
load("~/Downloads/cps_2212.rda")
## > sum(is.na(cps_2212))
is_na_age <- is.na(cps_2212$PRTAGE)
cps_2212 <- cps_2212[!is_na_age,]
cps <- cps_2212[!is.na(cps_2212$PRTAGE),]
OfAge <- cps[cps$PRTAGE > 15,]
cps_2212 <- OfAge
View(cps_2212)
cps <- OfAge
## unemployed <- OfAge[OfAge$PUWK == 2, OfAge$PULK > 1, ,]
searching<- OfAge$PELKAVL == 1
OfAge$employed <- OfAge$PUWK == 1
cps$notWorking <- cps$PUWK == 2
cps$AbleToWork <- cps$PELKAVL == 1
cps$NotLaidOff <- cps$PULAY == 2
cps$Looking <- cps$PULK == 1
cps$UnEmployed <- cps$notWorking & cps$AbleToWork & cps$NotLaidOff & cps$Looking
##
## PLACE CODE HERE
num_employed <- sum(cps$employed)
num_unemployed <- sum(cps$UnEmployed)
num_laborforce <- sum(num_unemployed,num_employed)
#This says that the unemployment rate is 2.12%, and the labor force is 56.54%
## PLACE CODE HERE
plot(cps$PRUNEDUR,cps$PEEDUCA)
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
cps <- cps_2212[!is.na(cps_2212$PRTAGE),]
cps <- cps[cps$PRTAGE > 15,]
## 44,902 rows were removed.
## PLACE CODE HERE
cps <- cps_2212[!is.na(cps_2212$PRTAGE),]
cps <- cps[cps$PRTAGE > 15,]
## 44,902 rows were removed.
## PLACE CODE HERE
cps$lookingForwork <- cps$PULK == 1
cps$Employed <- cps$PUWK == 1
cps$notWorking <- cps$PUWK == 2
cps$AbleToWork <- cps$PELKAVL == 1
cps$on_layoff <- cps$PULAY == 1
cps$unemployed <- cps$Employed & cps$notWorking & cps$AbleToWork | cps$on_layoff
## PLACE CODE HERE
num_unemployed <- sum(cps$unemployed)
num_employed <- sum(cps$Employed)
num_laborforce <- sum(num_employed, num_unemployed)
participation_rate <- num_laborforce / length(cps$PRTAGE)
## Participation rate - 55.69137%
unemployment_rate <- num_unemployed / num_laborforce
## unemployment rate - 0.63057%
load("~/Downloads/cps_2212.rda")
knitr::opts_chunk$set(echo = TRUE)
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
data_minusNA <- cps_2212[!is.na(cps_2212$PRTAGE),]
data_clean <- data_minusNA[data_minusNA$PRTAGE >=16,]
rows_removed <- nrow(cps_2212) - nrow(data_clean)
## PLACE CODE HERE
data_clean$PUWK <- ifelse(data_clean$PUWK ==1, TRUE, FALSE)
data_clean$PULK <- ifelse(data_clean$PULK ==1, TRUE, FALSE)
names(data_clean)[names(data_clean) =="PUWK"] <- "Work_Last_Week"
names(data_clean)[names(data_clean) =="PULK"] <- "Look_last_4weeks"
## PLACE CODE HERE
data_clean$UNEMPLOYED <- !data_clean$Work_Last_Week
Employed <-sum(data_clean$Work_Last_Week)
Unemployed <- sum(data_clean$UNEMPLOYED)
Labor_force <- Employed + Unemployed
unemployment_rate <- Unemployed / Labor_force
labor_force_participation_rate <- Labor_force / nrow(data_clean)
knitr::opts_chunk$set(echo = TRUE)
load("~/Downloads/cps_2212.rda")
load("~/Downloads/cps_2212.rda")
## PLACE CODE HERE
NAomit <-na.omit(cps_2212)
cps_adult <- NAomit [NAomit$PRTAGE >= 16 ,]
nrow(cps_2212) - nrow(cps_adult)
cps_adult$employed <- (cps_adult$PUWK == 1)
cps_adult$unemployed <- (cps_adult$PUWK == 2) | (cps_adult$PULAY == 1) | (cps_adult$PULK == 1) | (cps_adult$PELKAVL == 1)
cps_adult$employed
cps_adult$unemployed
cps_adult$employed <- (cps_adult$PUWK == 1)
cps_adult$unemployed <- (cps_adult$PUWK == 2) | (cps_adult$PULAY == 1) | (cps_adult$PULK == 1) | (cps_adult$PELKAVL == 1)
#cps_adult$employed
#cps_adult$unemployed
cps_adult$labor_force <- (cps_adult$employed | cps_adult$unemployed)
LF <- cps_adult [cps_adult$employed | cps_adult$unemployed ,]
cps_adult$unemployment_rate <- mean(cps_adult$unemployed) / mean(cps_adult$employed)
#laborr8 is the labor force participation rate
cps_adult$laborr8 <- mean(cps_adult$labor_force)
mean(cps_adult$unemployment_rate)
mean(cps_adult$laborr8)
## PLACE CODE HERE
#NN = No Negative 1 (-1)
NN <- LF$PRUNEDUR >= 0
Duration<- LF[NN, ]
mean(Duration$PRUNEDUR)
median(Duration$PRUNEDUR)
hist(Duration$PRUNEDUR)
#from looking at the data from the histogram the bar that stands out the most is that more than 800 people have been unemployed for less than (give or take) 10 weeks. Then from there it goes way down. The next spike in the bar occurs when around 100 people have been unemployed from around 50 to 60 weeks. The crazy part to me about this histagram is the amount of people from just this small frame of data we have who have been unemployed for over 100 weeks. But beoynd that this histogram has the distribution I expected from looking at the data.
HighSchool <- LF[LF$PEEDUCA == 39, ]
Bachelors <- LF[LF$PEEDUCA == 43, ]
Professional <- LF[LF$PEEDUCA == 45, ]
mean(HighSchool$unemployed)
mean(Bachelors$unemployed)
mean(Professional$unemployed)
# comparing the means of those in the labor force who either have a high school degree, bachelors degree, or professional degree, the results are pretty similar. As expected the lower the degree you have the higher the unemployment level. I thought the high school mean would be a bit higher but nowadays it is easier for people to get jobs that dont require higher education. Then when it comes to to Bachelor and Professional the numbers make sense especially because of Covid, a lot of people are out of work and still searching. But overall I would say these means fit the data.
load("~/Downloads/cps_2212.rda")
num_employed <- sum(cps_2212$Employed)
num_unemployed <- sum(cps_2212$Unemployed)
num_laborforce <- num_emoployed + num_unemployed
num_employed <- sum(cps_2212$Employed)
num_unemployed <- sum(cps_2212$Unemployed)
num_laborforce <- num_employed + num_unemployed
rete_unemploted <- num_unemployed / num_laborforce * 100
num_Is_16 <- sum(Is_16)
num_employed <- sum(cps_2212$Employed)
num_unemployed <- sum(cps_2212$Unemployed)
num_laborforce <- num_employed + num_unemployed
rete_unemploted <- num_unemployed / num_laborforce * 100
#num_Is_16 <- sum(Is_16)
laborforce_participate_rate <-num_laborforce / num_Is_16 *100
num_employed <- sum(cps_2212$Employed)
num_unemployed <- sum(cps_2212$Unemployed)
num_laborforce <- num_employed + num_unemployed
rete_unemploted <- num_unemployed / num_laborforce * 100
#num_Is_16 <- sum(Is_16)
#laborforce_participate_rate <-num_laborforce / num_Is_16 *100
## PLACE CODE HERE
Is_negative1 <- cps_2212$PRUNEDUR != -1
Mean_unemployed = Is_negative1 & cps_2212$Unemployed
result.mean <- mean(Mean_unemployed,)
result.median <- median(Mean_unemployed,)
## PLACE CODE HERE
Is_negative1 <- cps_2212$PRUNEDUR != -1
Mean_unemployed = Is_negative1 & cps_2212$Unemployed
result.mean <- mean(Mean_unemployed,)
result.median <- median(Mean_unemployed,)
## PLACE CODE HERE
load("~/Downloads/cps_2212.rda")
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
str_split("this/is/it", "/")
str_split("this/is/it", "/")[1]
str_split("this/is/it", "/")[[1]]
str_split("this/is/it", "/")[[2]]
str_split("this/is/it", "/")[1][2]
str_split("this/is/it", "/")[1][1]
str_split("this/is/it", "/")[[1]][1]
str_split("this/is/it", "/")[[1]]
str_split("this/is/it", "/")[[1]][1]
str_split("this/is/it", "/")[[1]][length(3)]
str_split("this/is/it", "/")[[1]][3]
?read_csv()
?read.csv
library(tidyverse)
?read.csv2
?read_csv
?ifelse
treated_sample <- nsw |>
filter(treated == 1)
seasonal_effects
?mean
reg_ar1 <- lm(ddr ~ ddr.l1, default_ts)
summary(reg_ar1)
immigration[[1]]
library(tidyverse)
library(lubridate)
by_year <- read_csv("nber_by_year.csv")
library(zoo)
?vline
library(ggplot2)
library(stringr)
?str_split
str_split("this is my life", " ")
str_split("this is my life", ",")
str_split("this is my life", "my")
str_split("this is my life", " ")
length(str_split("this is my life", " ")[[1]])
setwd("~/Jupyter/PatentPublish/03_RegressionAnalysis")
library(tidyverse)
library(lubridate)
library(stargazer)
library(ivreg)
patents <- read_csv("./Data/processed.csv")
count <- patents |>
group_by(assignee_id) |>
summarize(count = n())
cutoff <- quantile(count$count, probs = .75)
high_assignees <- count[count$count > cutoff, ]$assignee_id
low_assignees <- setdiff(count$assignee_id, high_assignees)
high <- patents[patents$assignee_id %in% high_assignees,]
low  <-  patents[patents$assignee_id %in% low_assignees,]
rm(patents, count)
# For HIGH
#==============================================================
# FIRST STAGE in IV for Relevance of Instrument
#==============================================================
first_stage_hc <- lm(change_nber ~
diff_growth + log(lag_citation),
data = high)
summary(first_stage_hc)
stargazer(first_stage_hc)
#==============================================================
# IV estimate
#==============================================================
iv_model_hc <- ivreg(log(num_citation) ~
change_nber + log(lag_citation)  |
log(lag_citation) + diff_growth,
data = high)
summary(iv_model_hc)
#===============================================================
# Comparison Model without IV
#===============================================================
no_iv_hc <- lm(log(num_citation) ~
log(lag_citation) + change_nber,
data = high)
summary(no_iv_hc)
# For LOW
#==============================================================
# FIRST STAGE in IV for Relevance of Instrument
#==============================================================
first_stage_lc <- lm(change_nber ~
diff_growth + log(lag_citation),
data = low)
summary(first_stage_lc)
stargazer(first_stage_lc)
#==============================================================
# IV estimate
#==============================================================
iv_model_lc <- ivreg(log(num_citation) ~
change_nber + log(lag_citation)  |
log(lag_citation) + diff_growth,
data = low)
summary(iv_model_lc)
#===============================================================
# Comparison Model without IV
#===============================================================
no_iv_lc <- lm(log(num_citation) ~
log(lag_citation) + change_nber,
data = low)
summary(no_iv_lc)
stargazer(first_stage_hc,first_stage_lc,first_stage_hs,first_stage_ls)
library(tidyverse)
library(lubridate)
library(stargazer)
library(ivreg)
library(xtable)
patents <- read_csv("./Data/processed.csv")
by_assignees <- patents |>
group_by(assignee_id) |>
summarize(
citation = mean(num_citation),
switches = sum(change_nber, na.rm = TRUE),
count = n()
)
by_assignees |>
select(-assignee_id) |>
mutate_all(log1p) -> corr_table
cutoff <- 1
high_assignees <- by_assignees[by_assignees$switches > cutoff, ]$assignee_id
low_assignees <- setdiff(by_assignees$assignee_id, high_assignees)
high <- patents[patents$assignee_id %in% high_assignees,]
low  <-  patents[patents$assignee_id %in% low_assignees,]
rm(patents, by_assignees)
# For HIGH
#==============================================================
# FIRST STAGE in IV for Relevance of Instrument
#==============================================================
first_stage_hs <- lm(change_nber ~
diff_growth + log(lag_citation),
data = high)
summary(first_stage_hs)
stargazer(first_stage_hs)
#==============================================================
# IV estimate
#==============================================================
iv_model_hs <- ivreg(log(num_citation) ~
change_nber + log(lag_citation)  |
log(lag_citation) + diff_growth,
data = high)
summary(iv_model_hs)
#===============================================================
# Comparison Model without IV
#===============================================================
no_iv_hs <- lm(log(num_citation) ~
log(lag_citation) + change_nber,
data = high)
summary(no_iv_hs)
# For LOW
#==============================================================
# FIRST STAGE in IV for Relevance of Instrument
#==============================================================
first_stage_ls <- lm(change_nber ~
diff_growth + log(lag_citation),
data = low)
summary(first_stage_ls)
stargazer(first_stage_ls)
#==============================================================
# IV estimate
#==============================================================
iv_model_ls <- ivreg(log(num_citation) ~
change_nber + log(lag_citation)  |
log(lag_citation) + diff_growth,
data = low)
summary(iv_model_ls)
#===============================================================
# Comparison Model without IV
#===============================================================
no_iv_ls <- lm(log(num_citation) ~
log(lag_citation) + change_nber,
data = low)
summary(no_iv_ls)
stargazer(first_stage_hc,first_stage_lc,first_stage_hs,first_stage_ls)
stargazer(iv_model_hc,iv_model_lc, iv_model_hs, iv_model_ls)
library(tidyverse)
by_year <- read_csv("nber_by_year.csv")
by_year <- by_year |>
drop_na() |>
filter(year != 2015) # NAs for most Patents
nbers <- unique(by_year$nber)
non_category <- by_year |>
mutate(is_category = nber == nbers[1]) |>
group_by(is_category, year) |>
summarise(count = sum(count)) |>
ungroup() |>
group_by(is_category) |>
mutate(
lag_count = lag(count),
growth = (count - lag_count)/ lag_count,
nber = nbers[1])
for (x in nbers[2:length(nbers)]) {
non <- by_year |>
ungroup() |>
mutate(is_category = nber == x) |>
group_by(is_category, year) |>
summarise(count = sum(count)) |>
ungroup() |>
group_by(is_category) |>
mutate(
lag_count = lag(count),
growth = (count - lag_count)/ lag_count,
nber = x)
non_category <- rbind(non_category, non)
}
library(tidyverse)
by_year <- read_csv("nber_by_year.csv")
by_year <- by_year |>
drop_na() |>
filter(year != 2015) # NAs for most Patents
nbers <- unique(by_year$nber)
non_category <- by_year |>
mutate(is_category = nber == nbers[1]) |>
group_by(is_category, year) |>
summarise(count = sum(count)) |>
ungroup() |>
group_by(is_category) |>
mutate(
lag_count = lag(count),
growth = (count - lag_count)/ lag_count,
nber = nbers[1])
for (x in nbers[2:length(nbers)]) {
non <- by_year |>
ungroup() |>
mutate(is_category = nber == x) |>
group_by(is_category, year) |>
summarise(count = sum(count)) |>
ungroup() |>
group_by(is_category) |>
mutate(
lag_count = lag(count),
growth = (count - lag_count)/ lag_count,
nber = x)
non_category <- rbind(non_category, non)
}
time_growth <- non_category |>
spread(key = is_category, value = growth)
in_cat <- time_growth |>
select(nber, `TRUE`,year) |>
drop_na()
out_cat <- time_growth |>
select(nber, `FALSE`, year) |>
drop_na()
cat_diff <- in_cat |>
left_join(out_cat, by = c("nber", "year")) |>
group_by("nber") |>
mutate(
diff = `TRUE` - `FALSE`,
diff.l1 = lag(diff),
diff.l2 = lag(diff, n=2),
diff.l3 = lag(diff, n=3))
ar1 <- lm(diff ~ diff.l1, cat_diff)
summary(ar1)
ar2 <- lm(diff ~ diff.l1 + diff.l2, cat_diff)
summary(ar2)
summary(ar3)
ar3 <- lm(diff ~ diff.l1 + diff.l2 + diff.l3, cat_diff)
summary(ar3)
library(stargazer)
stargazer(ar1, ar2, ar3)
